{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aead223-1efa-44dd-9725-bbe3c8e847c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d06a1b0afc42e86706020ebfbb4a9cb8",
     "grade": false,
     "grade_id": "Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# PDM - Assignment 4\n",
    "\n",
    "- **Topic:** Decision Making\n",
    "- **Assessment:** The assignment is graded (0-10).\n",
    "\n",
    "\n",
    "- **Deadline:** 15.12.2025\n",
    "- **Submitting: SUBMIT ONLY `assignment4.ipynb` TO BRIGHTSPACE**. \n",
    "\n",
    "\n",
    "## Instructions\n",
    "**Installation:** Follow the instructions in `installation_instructions.md` to setup your Python environment, if you haven't already.\n",
    "\n",
    "**Checking your solutions:** All questions/exercises in this notebook are followed by tests. Some are for you to check your solution, others are hidden for grading. To get any points, your answer/code needs to pass both sets of tests.\n",
    "\n",
    "## Information\n",
    "Please fill in your names and student numbers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabfcce-b53a-44e0-a381-ce1232d4a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "STUDENT_1_NAME = \"\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"\"\n",
    "\n",
    "STUDENT_2_NAME = \"\"\n",
    "STUDENT_2_STUDENT_NUMBER = \"\"\n",
    "\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f422b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bac79dc5d37dca613d58c821e17978e",
     "grade": false,
     "grade_id": "Instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment, we consider variants of a decision-making problem in which an agent must navigate a grid-world environment. We will encode this problem as a Markov Decision Process (MDP) and apply a variety of planning and learning techniques to solve this problem under increasingly challenging conditions.\n",
    "\n",
    "The assignment begins with a series of theoretical questions. Afterwards, you will dive into hands-on implementation of some of the algorithms that we have discussed in the lecture.\n",
    "\n",
    "The lecture covers what you need to know for this exercise. For a refresher on some basics, you can also check the following material:\n",
    "\n",
    "- [Probability Primer for Probabilistic Robotics (Cyrill Stachniss)](https://www.youtube.com/watch?v=JS5ndD8ans4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1e342",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54f0c8acb92a044d6779802a3044c7b0",
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id=\"theory\"></a>\n",
    "# Theoretical Questions\n",
    "The following multiple-choice questions cover fundamentals of decision making. Please enter your answer where requested and remove `raise NotImplementedError()` afterwards. Make sure that all cells where you added an answer run (shift + enter) without giving an error.\n",
    "\n",
    "## Question 1\n",
    "\n",
    "Consider three discrete random variables $X$, $Y$ and $Z$ with $X \\perp Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b4b92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67249032338a49cc65c65d220e35b338",
     "grade": false,
     "grade_id": "Q1-a-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  (0.5p) For each statement below, state if it is true or false.\n",
    "\n",
    "**A.** $P(X,Y) = P(X)P(Y)$\n",
    "\n",
    "**B.** $P(X | Y) = P(X)$\n",
    "\n",
    "**C.** $P(X | Y, Z) = P(X| Z)$\n",
    "\n",
    "**D.** $P(X, Y | Z) = P(X | Z) P(Y | Z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac3921",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1eebbcced18b90089a09a3df84739fce",
     "grade": false,
     "grade_id": "Q1-a-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "answer_1a = None # True or False\n",
    "answer_1b = None # True or False\n",
    "answer_1c = None # True or False\n",
    "answer_1d = None # True or False\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5980247a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "84ed382ba7d59ce646f517de42054c3d",
     "grade": false,
     "grade_id": "Q1-selftests",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "assert isinstance(answer_1a, bool), \"Answers must be a boolean value: True, False\"\n",
    "assert isinstance(answer_1b, bool), \"Answers must be a boolean value: True, False\"\n",
    "assert isinstance(answer_1c, bool), \"Answers must be a boolean value: True, False\"\n",
    "assert isinstance(answer_1d, bool), \"Answers must be a boolean value: True, False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd510990-7fdd-4a4a-a22c-33ab1337e797",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4384740b49bb15c89eae7e801c8fedb1",
     "grade": true,
     "grade_id": "Q1-test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86be4414-150b-49a5-98e4-7f7dacf32672",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "663b1c3b050199719763c379e1b778ec",
     "grade": false,
     "grade_id": "cell-b8d81ca228153408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Consider a simple game in which, at every time-step, a player has the option of rolling a fair 6-sided die. Each roll costs 1 EUR, and the player **must** roll the first round. Each time the player rolls the die, they have two possible actions:\n",
    "\n",
    "1. *Stop*: Stop playing by collecting the EUR value according to the value shown by the die, or\n",
    "2. *Roll*: Roll again, paying another 1 EUR\n",
    "\n",
    "\n",
    "This problem can be modeled as an MDP. The player initially starts in state *Start*, where the player only has one possible action: *Roll*. Based on the outcome of the die, say $i \\in {1, ..., 6}$, the player transitions into State $s_i$. Once a player decides to *Stop*, ther game is over, transitioning the palyer to the *End* state. Assume no discounting of rewards, i.e., $\\gamma = 1$.\n",
    "\n",
    "In this task we will investigate policy iteration to solve the MDP. As the problem is very small, please perform this computation manually (on paper). You can of course attempt verify your manual solution with one of the programmatic solutions we will develop in the second half of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b609f8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ee8a11d2903a667bc8cd1e853ea8bb5",
     "grade": false,
     "grade_id": "Q1-b-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1p) Question 2.1\n",
    "\n",
    "You are given an initial policy in the table below. Recall from the lecture that the first step for each round of policy iteration is to evaluate the given policy. Further, recall that the policy utility satisfies the equation\n",
    "\n",
    "$U^{\\pi}(s) = R(s, \\pi(s)) + \\sum_{s'} T(s' \\mid s, \\pi(s)) U^{\\pi}(s')$\n",
    "\n",
    "Compute these values manually and assign them to the utility variables below\n",
    "\n",
    "| State | Policy $\\pi(s)$ | Utility $U^{\\pi}(s)$;|\n",
    "| :---- | :------- | :-------------------------------- |\n",
    "| $s_\\text{start}$ | *Roll* | answer_2_1_utility_sstart|\n",
    "| $s_1$ | *Roll*   | answer_2_1_utility_s1             |\n",
    "| $s_2$ | *Roll*   | answer_2_1_utility_s2             |\n",
    "| $s_3$ | *Stop*   | answer_2_1_utility_s3             |\n",
    "| $s_4$ | *Stop*   | answer_2_1_utility_s4             |\n",
    "| $s_5$ | *Stop*   | answer_2_1_utility_s5             |\n",
    "| $s_6$ | *Stop*   | answer_2_1_utility_s6             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee1e7cd",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "690284df984cb8e3243c35b1dcb09a49",
     "grade": false,
     "grade_id": "Q1-b-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "answer_2_1_utility_sstart = None\n",
    "answer_2_1_utility_s1 = None\n",
    "answer_2_1_utility_s2 = None\n",
    "answer_2_1_utility_s3 = None\n",
    "answer_2_1_utility_s4 = None\n",
    "answer_2_1_utility_s5 = None\n",
    "answer_2_1_utility_s6 = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f475f98",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d2d89a20f4a72b3f6452c705a88ca65",
     "grade": false,
     "grade_id": "Q2_1-selftest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "assert isinstance(answer_2_1_utility_sstart, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s1, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s2, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s3, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s4, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s5, (int, float)), \"The answer must be a number\"\n",
    "assert isinstance(answer_2_1_utility_s6, (int, float)), \"The answer must be a number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1bb47-b14c-47a8-ae77-0ec724b8e2df",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e2e51ac982438e3f7f3cdf586dcc417e",
     "grade": true,
     "grade_id": "Q2_1-test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b04c21fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f63f224047c8c5cbbe62f847d6c6032",
     "grade": false,
     "grade_id": "Q2_2-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## (0.5p) Question 2.2\n",
    "\n",
    "Having determined the values, perform a policy update to find a new policy $\\pi'$.\n",
    "\n",
    "| State | Policy $\\pi(s)$ | Improved Policy $\\pi'(s)$ |\n",
    "| :---- | :------- | :------------- |\n",
    "| $s_\\text{start}$ | *Roll* | answer_2_2_action_sstart|\n",
    "| $s_1$ | *Roll*   | answer_2_2_action_s1 |\n",
    "| $s_2$ | *Roll*   | answer_2_2_action_s2 |\n",
    "| $s_3$ | *Stop*   | answer_2_2_action_s3 |\n",
    "| $s_4$ | *Stop*   | answer_2_2_action_s4 |\n",
    "| $s_5$ | *Stop*   | answer_2_2_action_s5 |\n",
    "| $s_6$ | *Stop*   | answer_2_2_action_s1       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08747e47",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7acc0a724a5ce6e3f01f10f465ad4677",
     "grade": false,
     "grade_id": "Q2_2-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "answer_2_2_action_sstart = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s1 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s2 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s3 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s4 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s5 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "answer_2_2_action_s6 = None # \"roll\", \"stop\", or \"roll or stop\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05b079-b919-452e-8de4-612968fca731",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "12d1ffdd85d4f846d8eb48c3744222a6",
     "grade": false,
     "grade_id": "Q2_2-selftest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "assert answer_2_2_action_sstart == \"roll\"\n",
    "assert answer_2_2_action_s1 in [\"roll\", \"stop\", \"roll or stop\"]\n",
    "assert answer_2_2_action_s2 in [\"roll\", \"stop\", \"roll or stop\"]\n",
    "assert answer_2_2_action_s3 in [\"roll\", \"stop\", \"roll or stop\"]\n",
    "assert answer_2_2_action_s4 in [\"roll\", \"stop\", \"roll or stop\"]\n",
    "assert answer_2_2_action_s5 in [\"roll\", \"stop\", \"roll or stop\"]\n",
    "assert answer_2_2_action_s6 in [\"roll\", \"stop\", \"roll or stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50074bf4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d138b5116af4ea70ee172386b2efb15",
     "grade": true,
     "grade_id": "Q2_2-test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6d10f03-e328-463f-b14b-4af858a81bed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a49871bb4c67b77f29f296a44f50b2c6",
     "grade": false,
     "grade_id": "Q2_3-questions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (0.5p) Question 2.3\n",
    "\n",
    "Is $\\pi(s)$ from Question 2.1 optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f98ec62-dc4c-4bf5-a6d5-da560de4096e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3538468d9d003d2e79107531f84549bd",
     "grade": false,
     "grade_id": "Q2_3-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "answer_2_3 = None # True or False\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48248a3d-b065-4d7e-8490-40270e8d1ec3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbfab7d9da5d66bf9de2ef073740c18e",
     "grade": false,
     "grade_id": "Q2_3-selftest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE OR MODIFY THIS CELL\n",
    "assert isinstance(answer_2_3, bool), \"answer must be bool: True or False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c5c5f-4479-44ca-8c95-5c5ec5b4d07c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "225b20847f15411f69c7b07fced11161",
     "grade": true,
     "grade_id": "Q2_3-test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac4abe42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7429fc2886cc32842fe5c17819f0fba4",
     "grade": false,
     "grade_id": "Q3-question",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## (0.5p) Question 3\n",
    "\n",
    "Recall the update equation in Q-learning:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\gets (1-\\alpha) Q(s, a) + \\alpha \\left(r + \\gamma \\max_{a'} Q(s', a')\\right)\n",
    "$$\n",
    "\n",
    "We have discussed in the lecture that a suitable choice of the learning rate $\\alpha$ is important for the success of this algorithm.\n",
    "\n",
    "In order to get a better idea why this parameter is important, it is instructive to think about what happens at extreme learning rates.\n",
    "\n",
    "For each statement below, indicate if it is true or false:\n",
    "\n",
    "- **A** For $\\alpha = 0$ the q-values never change; for $\\alpha = 1$ the agent immediately forgets old rewards\n",
    "- **B** For every MDP, there exists a fixed value $\\alpha \\in (0, 1)$ that ensures convergence to the optimal Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077819b8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "627a40408a043dd0d7d0cc9d04caa48e",
     "grade": false,
     "grade_id": "Q3-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "a = None # True or False\n",
    "b = None # True or False\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f6412-dd14-4d3b-9dd2-ac7db0d3f8ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "115bcf259816772f99428dcafcb03931",
     "grade": false,
     "grade_id": "Q3-selftest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL\n",
    "assert isinstance(a, bool), \"Answer must be a bool: True or False\"\n",
    "assert isinstance(b, bool), \"Answer must be a bool: True or False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d99032",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c0eb94ffc92eb4840592725e363173e",
     "grade": true,
     "grade_id": "Q3-test",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e157118-c751-467a-bbc4-19a0a99b7cb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98bb9f4b0928fe89f6d8c1b424c77ccf",
     "grade": false,
     "grade_id": "cell-06e00b9548cceab4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Implementation Exercises - Decision Making Under Uncertainty a Grid-World\n",
    "\n",
    "In the following exercise, we implement some of the decision making algorithms that we covered in the lectures.\n",
    "\n",
    "For this exercise, we consider different variants of a gridworld like the one shown below.\n",
    "In this setting, the agent is tasked to reach the goal square shown in green while avoiding the cliffs shown in red.\n",
    "Additionally, there are some \"obstacle regions\" which the agent cannot enter.\n",
    "At every time step, they can choose one of four actions: north, east, south, west.\n",
    "If an action is infeasible, i.e., moving into an obstacle or violating the outer boundary, the agent will simply remain at their current square.\n",
    "\n",
    "To make the problem more interesting, we will consider problem variants with different levels of transition noise.\n",
    "For a transition noise level of $\\eta = 0$, the actions of the agent are exact.\n",
    "That is, if their action is feasible, the agent will deterministically move in that direction.\n",
    "However, for a non-zero transition, the agent moves *orthogonal* to the planned with probability $\\eta$.\n",
    "If the noise pushes the robot into the wall or an obstacle, the robot will simply remain at their current square."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "903196e1-8666-4fa6-b29b-f729d0e6c283",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b1141f9be4253ed7b5b289484275ec2",
     "grade": false,
     "grade_id": "cell-3f781b393b421cb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![](resources/gridworld_explanation.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b2549-77e7-448a-bc21-931110e2795a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f1418ffc367be837565aecc7fdcda07",
     "grade": false,
     "grade_id": "cell-95793dda7ef25abd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have implemented a mathematical model of the problem above in a simple MDP data structure.\n",
    "For a complete description of this data type, please take a look at the file `mdp.py`.\n",
    "In this data type, actions and states are simply represented by an integer-valued index.\n",
    "Given an MDP object `problem`, you only have to interact with it through the following functions and properties\n",
    "\n",
    "**You can treat the internals of these functions as a black box. *Do not modify the problem definition***\n",
    "\n",
    "- `problem.num_states` is number of states of the problem\n",
    "- `problem.num_actions` is the number of actions of the problem\n",
    "- `problem.transition(state, action, next_state)` returns the probability of transitioning into `next_state` when applying `action` at the current `state`. If you omit the last argument, this function will return an array of transition probabilities for all states.\n",
    "- `poblem.sample_transition(state, action)` returns a sample, `(next_state, reward)`, which is drawn from the probability distributions specified by `problem.transition` and `problem.reward`, respectively.\n",
    "- `problem.reward(state, action, next_state)` return the reward for the transition tuple `(state, action, next_state)`. If you omit the argument, this function instead returns the *expected* reward over all possible next states from the given `(state, action)` pair.\n",
    "- `problem.discount_factor` the discount factor of the MDP\n",
    "\n",
    "**Note**\n",
    "\n",
    "The first two exercises are concerned with *planning*. In those exercises, we assume direct access to the transition and reward model of the underlying MDP. In those exercises, your algorithms will be allowed to call methods of the MDP directly. Towards the end of this exercise we will switch to *reinforcement learning* where this assumption is relaxed. That is, your final implementation will compute an optimal policy only based on *samples* collected in *interaction* with the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ae85e-639e-43d0-9adc-50e1ed13c9cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2fef82cecf1e5b11671edf02f7a9d11e",
     "grade": true,
     "grade_id": "cell-501d64f90b1d62f3",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "125e1454-2815-404a-89ab-c05320dc9d63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e938effb8ddeb19d545ccade5a37f031",
     "grade": false,
     "grade_id": "cell-96deb84deaf05bda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Offline Planning\n",
    "\n",
    "The first class of decision-making algorithms that we have discussed in the lecture are \"offline planning\" methods.\n",
    "As the name indicates, methods of this class compute a policy for all possible states they may encounter at runtime a-priori, i.e., off-line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4987ceee-3ecb-453b-8b66-3e5b23cb3e9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b7f2d67a7ef55b530e055198cb93b5f",
     "grade": false,
     "grade_id": "cell-2ac10b251b213f21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (2p) Exercise 1\n",
    "\n",
    "Below, we provide the basic skeleton of a class that implements a Q-value iteration planner.\n",
    "\n",
    "**Implement the missing functions of this class highlighted with `TODO: YOUR CODE HERE`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cb9da9-b49f-46b8-a25e-90f65bc7da75",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe1213bd4be32a15bb9455e3fe670e49",
     "grade": false,
     "grade_id": "cell-1471f7ad2a7afbd9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Reloads the python files outside of this notebook automatically\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import mdp\n",
    "import numpy as np\n",
    "import policies\n",
    "import test_utils\n",
    "from IPython.display import HTML # For animations in the notebook\n",
    "\n",
    "\n",
    "class QValueIterationPlanner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem: mdp.MarkovDecisionProcess,\n",
    "        convergence_tolerance: float = 1e-4,\n",
    "        maximum_number_of_iterations: int = 1000,\n",
    "    ):\n",
    "        self.problem = problem\n",
    "        self.convergence_tolerance = convergence_tolerance\n",
    "        self.maximum_number_of_iterations = maximum_number_of_iterations\n",
    "\n",
    "        # compute the optimal policy once, offline\n",
    "        self.qvalue_policy = self.qvalue_iteration(problem)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.qvalue_policy\n",
    "\n",
    "    def update_model(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    def qvalue_iteration(self, problem: mdp.MarkovDecisionProcess):\n",
    "        qvalue_table = np.zeros((problem.num_states, problem.num_actions))\n",
    "        converged = False\n",
    "        for iteration in range(self.maximum_number_of_iterations):\n",
    "            # get the state value table from the current q-value table for the subsequent Bellman backup\n",
    "            old_qvalue_table = qvalue_table.copy()\n",
    "            state_value_table = np.max(old_qvalue_table, axis=1)\n",
    "            # update q-values for all state-action pairs\n",
    "            for state in range(problem.num_states):\n",
    "                for action in range(problem.num_actions):\n",
    "                    \"\"\"\n",
    "                    TODO: YOUR CODE HERE\n",
    "                    \"\"\"\n",
    "                    # YOUR CODE HERE\n",
    "                    raise NotImplementedError()\n",
    "                    \"\"\"\n",
    "                    DON'T MODIFY THE CODE IN THIS CELL BELOW THIS LINE\n",
    "                    \"\"\"\n",
    "            # convergence check\n",
    "            if np.allclose(\n",
    "                qvalue_table, old_qvalue_table, atol=self.convergence_tolerance\n",
    "            ):\n",
    "                print(\"converged after {} iterations\".format(iteration))\n",
    "                converged = True\n",
    "                break\n",
    "\n",
    "        if not converged:\n",
    "            print(\"maximum number of iterations reached\")\n",
    "\n",
    "        return policies.QValuePolicy(qvalue_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24e57f-14aa-47bc-a87f-f7d5b60af5bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae004fd3605ab4bbb1bfc0639ae2da06",
     "grade": false,
     "grade_id": "E1-selftest",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to check that your solution works on a simple test problem\n",
    "# If these tests pass, the chances are quite high that your implementation is correct.\n",
    "# If they don't pass, you certainly need to fix it before you can more on\n",
    "# You may use the visualization of the simple problem below to help with debugging.\n",
    "test_utils.test_qvalue_iteration_planner(QValueIterationPlanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa69c5a-a452-4f57-842b-1b029824d2ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "908b3786f4047865e98430882cce8ec9",
     "grade": false,
     "grade_id": "cell-1a5ac49f9df4899d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In order to help with debugging, you can use these visualization of the optimal policy and value function:\n",
    "import gridworld\n",
    "simple_world = gridworld.Gridworld.setup_simple_world()\n",
    "simple_problem = simple_world.derive_mdp()\n",
    "agent = QValueIterationPlanner(problem=simple_problem)\n",
    "\n",
    "policy = agent.get_policy()\n",
    "print(policy.qvalue_table)\n",
    "simple_world.draw_policy(policy, on_value_function = False)\n",
    "simple_world.draw_policy(policy, on_value_function = True)\n",
    "\n",
    "states, *_ = simple_problem.simulate(agent, number_of_steps = 10, initial_state = 2)\n",
    "trajectory = simple_world.get_trajectory_from_state_sequence(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83089ea1-9ba2-4bc8-a257-3a84f930f8e9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a85c5aedf31927d92655435660589ff",
     "grade": true,
     "grade_id": "E1-test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1c8929d-c648-4f97-ba08-71a84941fcb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99958703a3f43b8b91f9577ef7196280",
     "grade": false,
     "grade_id": "cell-f85560d327109481",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (0.5p) Exercise 1 - Follow-Up Question 1\n",
    "\n",
    "Now that you have a working implementation of a first planning algorithm, we can use this planner to build some intuition about the impact of *noise* on the optimal policy. The code below invokes your solver on three variants of the same gridworld problem, each of them has a different noise level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd6dcf6-74fc-40ff-9e88-c45dd403285e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a313989388b540dd78444de40521453",
     "grade": false,
     "grade_id": "cell-03735a6394e4905b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import gridworld\n",
    "medium_world = gridworld.Gridworld.setup_medium_world()\n",
    "medium_world.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26601bb4-d774-4119-8aaf-81425b5ec8b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b96836ca93e68f40c2f0954b06e808f9",
     "grade": false,
     "grade_id": "cell-6b8f5f035eb8925f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**No Noise: $\\eta = 0$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d770e49-ca72-4997-9f47-47c16c92eae8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4de9a9442d0188e51936b068ecd6bcce",
     "grade": false,
     "grade_id": "cell-57dfa44143998586",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "medium_problem = medium_world.derive_mdp(transition_noise = 0)\n",
    "agent = QValueIterationPlanner(problem=medium_problem)\n",
    "policy = agent.get_policy()\n",
    "medium_world.draw_policy(policy, on_value_function = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303970d-7ae8-4c1f-9467-21c5f4deab54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8060481c9c981ef777c6fa1054203653",
     "grade": false,
     "grade_id": "cell-8364e4720288cb82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Moderate Noise: $\\eta = 0.1$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075d5ea-b6bb-453a-a864-d5a1c01026d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9181c74aa1bb86360e4f6f15870339ed",
     "grade": false,
     "grade_id": "cell-a3685664806d1708",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "medium_problem = medium_world.derive_mdp(transition_noise = 0.1)\n",
    "agent = QValueIterationPlanner(problem=medium_problem)\n",
    "policy = agent.get_policy()\n",
    "medium_world.draw_policy(policy, on_value_function = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1bd32e-46a7-4cad-8871-377c5b628e91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c923fdfe654164b30d011b3bab728a66",
     "grade": false,
     "grade_id": "cell-25cfa6973b2d0ec3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**High Noise: $\\eta = 0.5$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbe8fb-cbaa-4532-a440-2a6e71b7fa7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c45f5138941e68a966d24db21b33704",
     "grade": false,
     "grade_id": "cell-a8d4648305a19037",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "medium_problem = medium_world.derive_mdp(transition_noise = 0.5)\n",
    "agent = QValueIterationPlanner(problem=medium_problem)\n",
    "policy = agent.get_policy()\n",
    "medium_world.draw_policy(policy, on_value_function = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d92670-041f-4859-933e-2094c1e04f63",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f160f73e34a7a40ecd620cb3a5785682",
     "grade": false,
     "grade_id": "E1-Q1-questions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on these examples, for each statement below, indicate if it is true or false:\n",
    "\n",
    "- **A**: for larger values of $\\eta$, the optimal utility achievable by the agent goes down\n",
    "- **B**: for larger values of $\\eta$, the policy found by the q-value-iteration is no longer optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb707a-a89a-476b-9e67-6ff46a06dba8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6f8a72daf9469d0729e45919028101f",
     "grade": false,
     "grade_id": "E1-Q1-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a = None # True or False\n",
    "b = None # True or False\n",
    "\n",
    "# for larger values of Î· the optimal utility achievable by the agent goes down\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed042b-31ab-4666-b63e-161b3edf4e62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba6e7a73cc386ab7bf1de984dd1bfdde",
     "grade": false,
     "grade_id": "cell-61ac378fdef63aa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL\n",
    "assert isinstance(a, bool), \"Answer must be a bool: True or False\"\n",
    "assert isinstance(b, bool), \"Answer must be a bool: True or False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a85ae-de77-4856-9832-855d5b3cf9a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7238bf5bb1c57cd655b2ae5f251576df",
     "grade": true,
     "grade_id": "cell-f8023a64fff3452a",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2feb398c-8004-464a-a30f-bc4daad52053",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35ef65beb4f7667d7f1ddf818687228e",
     "grade": false,
     "grade_id": "cell-aa20cf385f0125be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next, we switch gears to *online planning* for the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1070e9-a4bd-4516-a4ca-ef465d1c1725",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0eda8035d84283883c0d43599e0e8fd",
     "grade": false,
     "grade_id": "cell-89dc4c10340086d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Online Planning\n",
    "\n",
    "In contrast to offline planning, online planning does not greedily compute the action for all possible states a-priori. Instead, it delays the decision which action to take until it encounters the state *at runtime*. As we have discussed in the lecture, this can be beneficial in settings where the statespace is very large. However, it also comes at additional challenges because the search space grows exponentially with depth of the search tree.\n",
    "\n",
    "In this exercise, we will explore these limitations of forward search and develop some ways to address them by making use of domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16204cac-a1b9-45c6-8524-c825f83f7db2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b586861b5002221ae26688b24b19fa59",
     "grade": false,
     "grade_id": "cell-fcc2150bd80353bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (2p) Exercise 2\n",
    "\n",
    "Below, we provide the basic skeleton of a class that implements a forward search planner.\n",
    "\n",
    "**Implement the missing functions of this class highlighted with `TODO: YOUR CODE HERE`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06659ba-29ec-410b-a636-e10cd1155707",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d010a2d002f7a496d9a4925dafd7a280",
     "grade": false,
     "grade_id": "cell-671551c48330d10f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ForwardSearchPlanner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        problem: mdp.MarkovDecisionProcess,\n",
    "        horizon=3,\n",
    "        utility_function_estimate=None,\n",
    "    ):\n",
    "        self.problem = problem\n",
    "        self.horizon = horizon\n",
    "        self.utility_function_estimate = utility_function_estimate\n",
    "\n",
    "    def get_policy(self):\n",
    "        return policies.OnlinePolicy(\n",
    "            lambda state: self.forward_search(state, self.horizon)\n",
    "        )\n",
    "\n",
    "    def update_model(self, state, action, next_state, reward):\n",
    "        pass\n",
    "\n",
    "    def forward_search(self, state, horizon):\n",
    "        # base case: if we have reached the horizon, then we return the user-provided utility function\n",
    "        # estimate at the current state\n",
    "        if horizon == 0:\n",
    "            if self.utility_function_estimate is None:\n",
    "                return None, 0\n",
    "            return None, self.utility_function_estimate(state)\n",
    "        # from the initial state, recursively expand the Bellman equation up to the horizon\n",
    "        # enumerate all possible actions we could take\n",
    "        best_action = None\n",
    "        best_action_value = -np.inf\n",
    "\n",
    "\n",
    "        for action in range(self.problem.num_actions):\n",
    "            # get the transition probabilities for all possible next states\n",
    "            transition_probabilities = self.problem.transition(state, action)\n",
    "            # get the indices of all the states that have non-zero transition probability\n",
    "            reachable_next_states = np.nonzero(transition_probabilities)[0]\n",
    "            \"\"\"\n",
    "            TODO: YOUR CODE HERE\n",
    "            \n",
    "            Hint: This stub is deriberately structured to allow your implementation to closely follow the\n",
    "            pseude code provided in the lecture. From that lecture, recall that forward search is implemented as\n",
    "            a *recursive* algorithm. That is, part of this function body will have to call `self.forward_search`\n",
    "            again (with a different set of arguments).\n",
    "            \"\"\"\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \"\"\"\n",
    "            DON'T MODIFY THE CODE IN THIS CELL BELOW THIS LINE\n",
    "            \"\"\"\n",
    "            if action_value > best_action_value:\n",
    "                best_action = action\n",
    "                best_action_value = action_value\n",
    "\n",
    "        return best_action, best_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3b288-37d8-4712-8b11-4cb3fccf1c66",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8560affb595c42d6ea5c08498f80754d",
     "grade": false,
     "grade_id": "cell-faac6d63db67ae23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to check that your solution works on a simple test problem\n",
    "# If these tests pass, the chances are quite high that your implementation is correct.\n",
    "# If they don't pass, you certainly need to fix it before you can more on\n",
    "\n",
    "test_utils.test_forward_search_planner(ForwardSearchPlanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f6017-f642-4923-8dea-b0a0f5495af6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06fe6c9f54620ec8e046dbbcc616635a",
     "grade": false,
     "grade_id": "cell-af0f30b819da11eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# You can use the visualizations below to help with debugging\n",
    "# Note: running this cell make take up to 1min\n",
    "import gridworld\n",
    "simple_world = gridworld.Gridworld.setup_simple_world()\n",
    "simple_problem = simple_world.derive_mdp()\n",
    "agent = ForwardSearchPlanner(problem=simple_problem, horizon = 6)\n",
    "\n",
    "policy = agent.get_policy()\n",
    "simple_world.draw_policy(policy, on_value_function = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e3575b-5893-4541-acfd-7509de39e041",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46ec5041d2af72af086f1dae24e30576",
     "grade": true,
     "grade_id": "cell-ff4f61ad20aad590",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037af898-72f0-4fe1-885f-6b8078c5438d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9091f097ff99d0babd6cabd23a8c0ca2",
     "grade": false,
     "grade_id": "cell-153c203d3b7c94bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "As we have discussed in the lecture, forward search has two important tuning parameters: the planning horizon and the utility estimate. The remainder of this section is designed for you to build a better intuition of these parameters.\n",
    "\n",
    "### (0.5p) Exercise 2 - Follow-Up Questions 1\n",
    "\n",
    "The code cell below visualizes the policy and utility estimate computed by forward search for two different planning horizons, $h \\in \\{3, 4\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dcede-aa99-4c2c-9af2-ddf4ce619dd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c888177ef9859967dba1f986cefe574",
     "grade": false,
     "grade_id": "cell-998327cf61dcd441",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import gridworld\n",
    "medium_world = gridworld.Gridworld.setup_medium_world()\n",
    "medium_problem = medium_world.derive_mdp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dac6c6-af11-41d1-8c86-1423e780bedf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ea70f352754cff322569d5e672f5c40",
     "grade": false,
     "grade_id": "cell-55e3633948b09dc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = ForwardSearchPlanner(problem=medium_problem, horizon = 3)\n",
    "policy = agent.get_policy()\n",
    "medium_world.draw_policy(policy, on_value_function = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46830c97-4906-4953-b1e8-c95059e1dbbe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57bbcc8b1285f81cc1b3432c6f36f21b",
     "grade": false,
     "grade_id": "cell-25f4b4bdba9b90ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "agent = ForwardSearchPlanner(problem=medium_problem, horizon = 4)\n",
    "policy = agent.get_policy()\n",
    "medium_world.draw_policy(policy, on_value_function = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1c316-dc5f-4e92-97f4-a798eda17dc9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56c93f9b91e7e04f19502f8b37e025d",
     "grade": false,
     "grade_id": "cell-9efd8ac2f00785ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the visualization above, and the discussion in the lecture, indicate for each of the following statements whether they are true or false\n",
    "\n",
    "- **A**: A longer planning horizon improves the quality of the policy and allows the robot to find the goal from larger distances\n",
    "- **B**: For a horizon of $h$, the first $h$ actions of the agent are always optimal.\n",
    "- **C**: There exists a utility estimate that ensures optimality of forward search irrespective of the planning horizon.\n",
    "- **D**: The agent must only run forward-search once to solve an MDP and can follow the thus computed plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da1811-b5e7-447d-8765-ea7550bdcc2d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a73492c9fa1ead02d9d4c6c1ae8fdf87",
     "grade": false,
     "grade_id": "cell-8897fe445a8732b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a = None # True or False\n",
    "b = None # True or False\n",
    "c = None # True or False\n",
    "d = None # True or False\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad4db6-6d78-4999-9d9f-7845e2fa3d75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d868edb851947bb7ac6195cfdf16628",
     "grade": false,
     "grade_id": "cell-6ce0b5cb53903bae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL\n",
    "assert isinstance(a, bool), \"Answer must be a bool: True or false\"\n",
    "assert isinstance(b, bool), \"Answer must be a bool: True or false\"\n",
    "assert isinstance(c, bool), \"Answer must be a bool: True or false\"\n",
    "assert isinstance(d, bool), \"Answer must be a bool: True or false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb25ce-61e0-4fcc-9520-e48aa2d20574",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba898419aba4dc71f87ef67de044b199",
     "grade": true,
     "grade_id": "cell-fec94d879fbe3524",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92100c46-3fac-44fa-81ab-ea16c4dd53d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bf7ec974ab145a319ac43da0f4a4d03",
     "grade": false,
     "grade_id": "cell-e6a680541e2670ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Finally, we also want to explore decision-making in MDPs when the transition function $T$ and reward function $R$ of the environment are unknown a-priori.\n",
    "As we have learned in the lecture, we can use reinforcement learning to recover an optimal policy in this setting from interaction with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fa40c8-9d04-4ed7-8e50-2283506afec6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3466c4c38ee776bc420ec2647fefa895",
     "grade": false,
     "grade_id": "cell-1137ad776b98c5fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (1.5p)  Exercise 3\n",
    "\n",
    "Below, we provide the basic skeleton of a class that implements a Q-learning.\n",
    "\n",
    "**Implement the missing functions of this class highlighted with `TODO: YOUR CODE HERE`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e92456-d42e-4867-997d-8c12a10e62d6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e52098b2215956abcfc36d605f2d3b4b",
     "grade": false,
     "grade_id": "cell-42c7fe49eda5f143",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from policies import EpsilonGreedyPolicy, QValuePolicy\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(\n",
    "        self, num_states, num_actions, discount_factor, learning_rate=0.01, epsilon=0.5\n",
    "    ):\n",
    "        qvalue_table = np.zeros((num_states, num_actions))\n",
    "        self.qvalue_policy = QValuePolicy(qvalue_table)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_policy = EpsilonGreedyPolicy(self.qvalue_policy, epsilon)\n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.exploration_policy.epsilon = epsilon\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self.exploration_policy\n",
    "\n",
    "    def update_model(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        YOUR CODE HERE\n",
    "        \n",
    "        Hint: You will have to update `self.qvalue_policy.qvalue_table` according to the q-learning update\n",
    "        rule discussed in the lecture. The relevenat parameters for discount factor and learning rate are provided by\n",
    "        `self.discount_factor` and `self.learning_rate`, respectively. This implementation can be done in a single line\n",
    "        of code so don't think too complicated.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d3dd5-3393-41b0-8ff9-5b9e641161fd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75ef70c19e14d2964e7808b4b9deb20",
     "grade": false,
     "grade_id": "cell-d434c6c4a9545da3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to check that your solution works on a simple test problem\n",
    "# If these tests pass, the chances are quite high that your implementation is correct.\n",
    "# If they don't pass, you certainly need to fix it before you can more on\n",
    "# You may use the visualization of the simple problem below to help with debugging.\n",
    "test_utils.test_qlearning(QLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1361644-5c5e-4c51-8b5d-8f9502e4b97d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b327639372b60f32ccc40c460e3be5e",
     "grade": true,
     "grade_id": "cell-fa972cd6fa4b5f98",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f9d78-1b04-41ae-9822-e5a691c37eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da497fc079253cc3abcdf6c2603139a0",
     "grade": false,
     "grade_id": "cell-b32550717d90cd06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In order to help with debugging, you can use these visualization of the optimal policy and value function:\n",
    "test_utils.visualize_qlearning(QLearning, gridworld.Gridworld.setup_simple_world())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777b5b0e-c751-454f-91e1-8764ceb5fdac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c5cfeb3852ea103f56b2a0572ec85214",
     "grade": false,
     "grade_id": "cell-0d681429cfd9cb82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### (0.5p) Exercise 3 - Follow-Up Question 1\n",
    "\n",
    "In the theory section of this assignment, we have already discussed on of the main parameters of Q-learning, the learning rate. Now, we want to build intuition for the second important design parameter: the exploration strategy.\n",
    "\n",
    "As you can see in the class stub of the `QLearning` agent, we provide the agent with a simple $\\epsilon$-greedy exploration strategy. With probability $\\epsilon$, this strategy chooses a random action rather than the current best estimate.\n",
    "\n",
    "To get a better sense for the impact of this parameter, the code below trains your Q-Learning agent on the medium sized world for two extreme values of the exploration parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fcc793-1def-4438-941d-e978c8ca6380",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9172d11e54d18261b7de7a0d8d41fbf",
     "grade": false,
     "grade_id": "cell-97e524e18a4059f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$\\epsilon = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2474fe-f41d-4431-bb81-ea5deb4aa114",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d8c286991a3c2986ff48f874a07d705",
     "grade": false,
     "grade_id": "cell-cfdd36d396265a12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_utils.visualize_qlearning(QLearning, gridworld.Gridworld.setup_medium_world(), number_of_steps_per_episode = 20, epsilon = 0.0, show_animation = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc817544-c7e4-4f62-a7fd-f2af0054cbaf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bf01edabeda19522deeeb6657cbaa26",
     "grade": false,
     "grade_id": "cell-4ca27e8909d7f8bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "$\\epsilon = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0345fe-11ba-4f26-b43a-a0dce4b1de5f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7df67c0df230fd1a8cd12dce2d96d8a7",
     "grade": false,
     "grade_id": "cell-70299ca47d55453e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_utils.visualize_qlearning(QLearning, gridworld.Gridworld.setup_medium_world(), number_of_steps_per_episode = 20, epsilon = 1, show_animation = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c92c8fd-7a84-4f5f-ae21-39e645c4510d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c0f9fc6aab9528b45c81ae6a219f97c",
     "grade": false,
     "grade_id": "cell-e4d1b67b5fa785fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Based on the observations above and the discussion in the lecture, indicate which of the following statements are correct:\n",
    "\n",
    "- **A**: For $\\epsilon = 0$, the learning agent only ever tries the best action according to its current q-table.\n",
    "- **B**: For $\\epsilon = 1$, the policy always chooses random actions ignores the q-values estimates entirely\n",
    "- **C**: Q-learning cannot find an optimal policy with $\\epsilon = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518a2781-d691-4a3c-93ac-f41064b5e8a3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "117d0e8b764d3c364cd01deefb2c5f30",
     "grade": false,
     "grade_id": "cell-178694ee85439c04",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "a = None # True or False\n",
    "b = None # True or False\n",
    "c = None # True or False\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ca57b-abfd-4e2a-9583-b1b6128bf546",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b52e089afe2e8d7b0dfb9294b68ca369",
     "grade": false,
     "grade_id": "cell-dfaa6274032be69a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT REMOVE THIS CELL\n",
    "assert isinstance(a, bool), \"Answer must be a bool: True or False\"\n",
    "assert isinstance(b, bool), \"Answer must be a bool: True or False\"\n",
    "assert isinstance(c, bool), \"Answer must be a bool: True or False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b3700-0983-4014-b5c8-53c9a9898bab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95064f62e376992ae96308fec3e3e4de",
     "grade": true,
     "grade_id": "cell-061beee33f7a0491",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
